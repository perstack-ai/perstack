## Description

Enhance Ollama provider with error handling, hybrid reasoning model support, and embedding model capabilities.

Related to #225, #190, #191, #192

### Use Case

Improve Ollama provider with:
- Proper error normalization for Ollama-specific errors
- Retry logic for Ollama connection issues
- Hybrid reasoning model support (e.g., qwen3)
- Embedding model support via textEmbeddingModel()

### Current State

The @perstack/ollama-provider package has minimal implementation:
- Only createModel() is implemented
- No normalizeError() / isRetryable() implementations
- No reasoning configuration support
- No embedding model support

### Target State

Enhanced provider adapter implementation:
1. **errors.ts**: Normalize Ollama errors and determine retry eligibility
2. **provider-options.ts**: Support for hybrid reasoning toggle
3. **adapter.ts**: Wire up error handling and provider options

### Implementation Notes

**No Provider Tools**: Ollama does not provide provider-defined tools. The getProviderTools() method will return an empty object (default behavior).

**Hybrid Reasoning Models**: Some Ollama models (e.g., qwen3) support toggling reasoning between messages:
```typescript
providerOptions: { ollama: { think: true } }
```
This allows dynamic control of whether the model should reason through the problem.

**Local Deployment**: Ollama runs locally, so error handling should focus on:
- Connection errors (service not running)
- Model not found errors (model not pulled)
- Resource exhaustion (memory, GPU)

**Embedding Models**: Ollama supports embedding models via textEmbeddingModel():
```typescript
const model = ollama.textEmbeddingModel('nomic-embed-text')
```
Consider whether to expose this capability through the adapter.

Refer to: https://ai-sdk.dev/providers/community-providers/ollama

### Proposed Solution

1. Create errors.ts with Ollama-specific error handling:
   - Handle connection errors (ECONNREFUSED, ENOTFOUND)
   - Handle model errors (model not found, invalid model)
   - Handle resource errors (out of memory)
   - Implement retry logic for transient connection issues

2. Create provider-options.ts for Ollama options:
   - think: boolean for hybrid reasoning models

3. Update adapter.ts:
   - Override normalizeError() and isRetryable()
   - Override getProviderOptions() for reasoning toggle
   - Consider adding createEmbeddingModel() for embedding support

### Configuration Example

```toml
[provider]
providerName = "ollama"

[provider.config]
baseUrl = "http://localhost:11434/api"

# For hybrid reasoning tasks
[experts.thinking-assistant]
modelId = "qwen3:4b"

[experts.thinking-assistant.providerOptions.ollama]
think = true

# For general chat
[experts.chat-assistant]
modelId = "llama3.2"
```

### Acceptance Criteria

- [ ] Error normalization handles Ollama-specific errors
- [ ] Connection errors are properly detected and reported
- [ ] Model not found errors provide helpful messages
- [ ] Retry logic handles transient connection issues
- [ ] Hybrid reasoning toggle works via providerOptions.ollama.think
- [ ] Documentation clarifies no provider tools are available
- [ ] Unit tests added
- [ ] E2E test added (with local Ollama instance)
